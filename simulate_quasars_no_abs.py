"""
Simulate quasar continuum and emission lines using `simqso` without any absorber templates.

This script generates synthetic quasar spectra with random properties.
The power-law slopes of the continuum model are varied using a gaussian distribution
of the slopes:
    GaussianSampler(-1.7, 0.1) and GaussianSampler(-0.3, 0.2)
with a break at 1215 Å.

The luminosities are randomly generated by sampling a random black hole mass
and a random Eddington ratio from log-normal distributions:
    M_BH : GaussianSampler(8.67, 0.5)
    R_Edd = GaussianSampler(-0.83, 0.4)

The rest-frame luminosity density at 1450Å is then calculated by assuming
a bolometric correction of 5 (Richards et al. 2006).

Fe lines are included using a re-scaled template of the Vestergaard & Wilkes (2001)
template. Lastly, broad lines are included following the Baldwin effect.

Optional broad absorption line (BAL) features can be added if requested.

The script generates a list of template input parameters in the output directory
with the following parameters:
    ID: quasar model id
    z: quasar redshift
    absMag: absolute magnitude (at 1450Å)
    smcDustEBV: E(B-V) in mag following the SMC extinction curve
    LOG_MBH: log of black hole mass in units of M_sun
    LOG_REDD: log of Eddington ratio
"""

from astropy.cosmology import Planck13
from astropy.table import Table, vstack
from astropy.io import fits
import numpy as np
from scipy import stats
import os
import glob
import gc
import pickle
import pandas as pd

from simqso.sqgrids import *
from simqso import sqbase
from simqso.sqrun import buildSpectraBulk, buildQsoSpectrum
from tqdm import tqdm

col_format_all_S17 = {
    'NAME':pd.StringDtype(),
    'RA':np.float64, 'DEC':np.float64,
    'PMRA':np.float32, 'PMDEC':np.float32,
    'EPOCH':np.float32, 'RESOLUTION':np.int16,
    'SUBSURVEY':pd.StringDtype(),
    'TEMPLATE':pd.StringDtype(), 
    'RULESET':pd.StringDtype(),
    'EXTENT_FLAG':np.int32,
    'EXTENT_PARAMETER':np.float32,'EXTENT_INDEX':np.float32,
    'MAG_TYPE':pd.StringDtype(),
    'MAG':np.float32, 'MAG_ERR':np.float32,
    'DATE_EARLIEST':np.float64, 'DATE_LATEST':np.float64,
    'CADENCE':np.int64,
    'REDDENING':np.float32,
    'REDSHIFT_ESTIMATE':np.float32,
    'REDSHIFT_ERROR':np.float32,
    'CAL_MAG_ID_BLUE':pd.StringDtype(),
    'CAL_MAG_ID_GREEN':pd.StringDtype(),
    'CAL_MAG_ID_RED':pd.StringDtype(),
    'CAL_MAG_ERR_BLUE':np.float32,
    'CAL_MAG_ERR_GREEN':np.float32,
    'CAL_MAG_ERR_RED':np.float32,
    'CAL_MAG_BLUE':np.float32,
    'CAL_MAG_GREEN':np.float32,
    'CAL_MAG_RED':np.float32,
    'CLASSIFICATION':pd.StringDtype(),
    'CLASS_SPEC':pd.StringDtype(),
    'COMPLETENESS':np.float32,
    'PARALLAX':np.float32,
    'SWEEP_NAME':pd.StringDtype(), 
    'BRICKNAME':pd.StringDtype(), 
    'TYPE':pd.StringDtype(), 
    'BAND_LEGACY':pd.StringDtype(), 
    'REFERENCE_BAND':pd.StringDtype(), 
    'COMBINATION_USE':pd.StringDtype(), 
    'REDSHIFT_REF':pd.StringDtype(), 
    'EBV':np.float64, 
    'PLXSIG': np.float64, 
    'PMSIG': np.float64, 
    'SN_MAX': np.float64, 
    'MAG_G': np.float32, 
    'MAGERR_G': np.float32, 
    'MAG_R': np.float32, 
    'MAGERR_R': np.float32, 
    'MAG_I': np.float32, 
    'MAGERR_I': np.float32, 
    'MAG_Z': np.float32, 
    'MAGERR_Z': np.float32, 
    'MAG_Y': np.float32, 
    'MAGERR_Y': np.float32, 
    'MAG_J': np.float32, 
    'MAGERR_J': np.float32, 
    'MAG_H': np.float32, 
    'MAGERR_H': np.float32, 
    'MAG_K': np.float32, 
    'MAGERR_K': np.float32, 
    'MAG_W1': np.float32, 
    'MAGERR_W1': np.float32, 
    'MAG_W2': np.float32, 
    'MAGERR_W2': np.float32, 
    'SPECTYPE_DESI': pd.StringDtype()
    }

col_units = {
    "RA": "deg", "DEC": "deg", "PMRA": "mas/yr", "PMDEC": "mas/yr",
    "EPOCH": "yr", "MAG": "mag", "MAG_ERR": "mag", "EXTENT_PARAMETER": "arcsec",
    "DATE_EARLIEST": "d", "DATE_LATEST": "d", "REDDENING": "mag",
    "CAL_MAG_BLUE": "mag", "CAL_MAG_GREEN": "mag", "CAL_MAG_RED": "mag",
    "CAL_MAG_ERR_BLUE": "mag", "CAL_MAG_ERR_GREEN": "mag", "CAL_MAG_ERR_RED": "mag",
    "PARALLAX": "mas",
}

def cols_format_dict(format_dict, dataframe):
    matching_columns = {}
    
    for col in dataframe.columns:
        if col in format_dict:
            matching_columns[col] = format_dict[col]
    
    return matching_columns

def format_pd_for_fits(df):
    
    df_copy = df.copy()
    
    for col_name in df_copy.columns:  # object to string

        col_values = df_copy[col_name].values

        if col_values.dtype == 'object':
            df_copy[col_name] = df_copy[col_name].astype(pd.StringDtype())

    format_cols = cols_format_dict(col_format_all_S17, df_copy)
    df_copy = df_copy.astype(format_cols)

    for col_name in df_copy.columns:  # fill empty cells

        col_series = df_copy[col_name].values

        if pd.api.types.is_string_dtype(df_copy[col_name]) or isinstance(col_series.dtype, pd.StringDtype):
            df_copy[col_name] = df_copy[col_name].fillna('-')
        else:
            if col_name in ['MAG_Z', 'MAG', 'MAGERR_Z', 'MAG_ERR', 'MAG_G', 'CAL_MAG_BLUE', 
                            'MAGERR_G', 'CAL_MAG_ERR_BLUE', 'MAG_R', 'CAL_MAG_GREEN', 'MAGERR_R', 'CAL_MAG_ERR_GREEN', 
                            'MAG_I', 'CAL_MAG_RED', 'MAGERR_I', 'CAL_MAG_ERR_RED']:
                df_copy[col_name] = df_copy[col_name].fillna(1.0)
            else:
                df_copy[col_name] = df_copy[col_name].fillna(-999)
    
    df_copy.reset_index(drop=True, inplace=True)
    return df_copy

def save_to_fits(df, filepath, meta=None):

    df_for_fits = format_pd_for_fits(df)
    
    t = Table()

    format_cols = cols_format_dict(col_format_all_S17, df_for_fits)
    for col_name in df_for_fits.columns:
        if col_name in format_cols.keys():
            col_data = df_for_fits[col_name].astype(col_format_all_S17[col_name])
            col_data = col_data.values
        else:
            col_data = df_for_fits[col_name].values

        if hasattr(col_data, 'values'):
            t[col_name] = col_data.values
        else:
            t[col_name] = [x for x in col_data]
            
    if meta:
        t.meta.update(meta)

    t.write(filepath, format='fits', overwrite=True)

def pandas_from_fits(filepath):
    t = Table.read(filepath, format='fits')
    
    t = t.to_pandas()

    format_cols = cols_format_dict(col_format_all_S17, t)
    t = t.astype(format_cols)

    return t

# ------------------------------------------------------------------------------------------------------------

__author__ = 'Jens-Kristian Krogager, modified'


here = os.path.abspath(os.path.dirname(__file__))

# def load_bal_templates():
#     """
#     Load all the rest-frame BAL templates into a dictionary.
#     """
#     models = {}
#     temp_fnames = {'felobal': os.path.join(here, 'BALs/felobal_ot_pro1_spec.pkl'),
#                    'hibal_1': os.path.join(here, 'BALs/hibal_1_pro1_spec.pkl'),
#                    'hibal_2': os.path.join(here, 'BALs/hibal_2_pro1_spec.pkl'),
#                    }
#     for key, fname in temp_fnames.items():
#         with open(fname, 'rb') as pkl_file:
#             wl, trans = pickle.load(pkl_file)
#             models[key] = (wl, trans)
#     return models

def simulate_quasars(nqso=None, z_list=None, names_list=None, z_range=(1.0, 4.5), 
                     wavelen_grid='/data2/home2/nguerrav/TNG50_spec/npy_files/TNG50_wavelength_grid_extended.npy', wave_range=(3000, 11000), 
                     dust_mode='exponential', #BAL=False, 
                     output_dir='/data2/home2/nguerrav/QSO_simpaqs/QSO_simpaqs/QSOs_full_cat'):
    """
    Simulate a set of quasars without any absorber templates.
    
    Parameters
    ----------
    nqso : int
        Number of quasars to simulate
    z_list : array-like, optional
        List of specific redshifts to use for the quasars. If provided, nqso is set to len(z_list)
    names_list : array-like, optional
        List of names corresponding to each quasar
    z_range : tuple
        Redshift range (min, max) to sample from
    wave_range : tuple
        Wavelength range in Angstroms
    dust_mode : str
        Dust sampling mode: 'exponential' or 'uniform'
    BAL : bool
        Whether to include broad absorption line features
    output_dir : str
        Output directory for the simulated quasar spectra
    
    Returns
    -------
    subset : astropy.table.Table
        Table with quasar parameters
    """

    # if BAL:
    #     bal_models = load_bal_templates()

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if wavelen_grid is not None:
        wave = np.load(wavelen_grid)
    else:
        wave = np.arange(wave_range[0], wave_range[1], 0.1)
        # TNG wavelength grid here

    if z_list is not None:
        z_all = np.array(z_list)
        nqso = len(z_all)
        print(f"Using {nqso} provided redshifts")
    else:
        z_all = np.random.uniform(z_range[0], z_range[1], nqso)

    # Sampling MBH and LEdd from Rakshit, Stalin & Kotilainen (2020)
    logM_BH = np.random.normal(8.67, 0.5, nqso)
    logR_Edd = np.random.normal(-0.83, 0.4, nqso)
    logL_bol = np.log10(3.2e4) + logM_BH + logR_Edd
    BC = 5.  # Richards et al. (2006)
    M_all = 4.8 - 2.5*logL_bol + 2.5*np.log10(BC)

    M = AbsMagVar(FixedSampler(M_all), restWave=1450)
    z = RedshiftVar(FixedSampler(z_all))
    
    # Include dust sampled from E(B-V)
    if dust_mode.lower() == 'exponential':
        # based roughly on Krawczyk et al. 2015
        Ebv_all = stats.expon.rvs(loc=0., scale=0.05, size=nqso)
    else:
        Ebv_all = stats.uniform.rvs(0., 0.5, size=nqso)
    dust = SMCDustVar(FixedSampler(Ebv_all))

    qsos = QsoSimPoints([M, z, dust], cosmo=Planck13, units='luminosity')

    # use the canonical values for power law continuum slopes in FUV/NUV, with breakpoint at 1215A
    contVar = BrokenPowerLawContinuumVar([
        GaussianSampler(-1.7, 0.1),  # slope before break
        GaussianSampler(-0.3, 0.2)],  # slope after break
        [1215.]  # breakpoint is fixed --> GaussianSampler(1215., 5.)?
        )

    # generate lines using the Baldwin Effect emission line model from BOSS DR9
    emLineVar = generateBEffEmissionLines(qsos.absMag)  # lines listed in /home/nguerrav/4MOST_like_data/simqso/simqso/data
                                                        # for each line: wavelength, logEW, logWidth
    # print('emLineVar:', emLineVar()[0][0], '\n')

    # the default iron template from Vestergaard & Wilkes 2001 was modified to fit BOSS spectra
    fescales = [(0, 1540, 0.5),
                (1540, 1680, 2.0),
                (1680, 1868, 1.6),
                (1868, 2140, 1.0),
                (2140, 3500, 1.0)]
    feVar = FeTemplateVar(VW01FeTemplateGrid(qsos.z, wave, scales=fescales))
    # Default velocity broadening = 5000 km/s FWHM

    # Now add the features to the QSO grid
    qsos.addVars([contVar, emLineVar, feVar])

    # print('qsos.data before: \n', qsos.data)

    # ready to generate spectra
    meta, spectra = buildSpectraBulk(wave, qsos, saveSpectra=True)

    print("Creating quasar models:")

    # Save the templates:
    all_ids = []
    # all_bal_types = []
    dnum = len(glob.glob(f'{output_dir}/QSOs_*.fits')) + 1
    for num, spec in enumerate(tqdm(spectra)):
        z_str = str(np.round(z_all[num], 4))#.replace('.', '_')

        if names_list is not None:
            target_name = names_list[num]
            model_id = f'QSO_sim_z{z_str}_{target_name}'
        else:
            model_id = f'QSO_z{z_str}_{num+dnum:06}'

        filename = f'{output_dir}/{model_id}.fits'
        all_ids.append(model_id)
        
        # bal_type = 'none'
        # if BAL:
        #     bal_type = np.random.choice(['hibal_1', 'hibal_2', 'felobal', 'none'],
        #                                 p=[0.2, 0.2, 0.2, 0.4])
        #     if bal_type != 'none':
        #         # Include a random BAL template
        #         bal_wl, models = bal_models[bal_type]
        #         model_num = np.random.randint(0, len(models))
        #         if len(models[model_num]) == 2:
        #             _, trans = models[model_num]
        #         else:
        #             trans = models[model_num]
        #         bal_trans = np.interp(wave, bal_wl*(z_all[num] + 1), trans)
        #         spec = spec * bal_trans
        # all_bal_types.append(bal_type)

        hdu = fits.HDUList()
        hdr = fits.Header()
        hdr['AUTHOR'] = 'Simulated quasar without absorbers'
        hdr['COMMENT'] = 'Synthetic quasar model based on simqso'
        hdr['REDSHIFT'] = z_all[num]
        hdr['MAG'] = (M_all[num], "Abs. magnitude 1450")
        hdr['EBV'] = (Ebv_all[num], "Mag")
        hdr['LOG_MBH'] = (logM_BH[num], "log(M_BH / Msun)")
        hdr['LOG_REDD'] = (logR_Edd[num], "log(R_Edd)")
        hdr['ID'] = model_id
        # hdr['BAL_TYPE'] = bal_type
        prim = fits.PrimaryHDU(header=hdr)
        hdu.append(prim)
        col_wl = fits.Column(name='LAMBDA', array=wave, format='1D', unit='Angstrom')
        col_flux = fits.Column(name='FLUX_DENSITY', array=spec, format='1E', unit='erg/(s cm**2 Angstrom)')
        tab = fits.BinTableHDU.from_columns([col_wl, col_flux])
        tab.name = 'TEMPLATE'
        hdu.append(tab)
        hdu.writeto(filename, overwrite=True, output_verify='silentfix')

    qsos.data['ID'] = all_ids
    qsos.data['LOG_MBH'] = logM_BH
    qsos.data['LOG_REDD'] = logR_Edd
    # qsos.data['BAL_TYPE'] = all_bal_types
    # qsos.data['']

    if names_list is not None:
        qsos.data['NAME'] = names_list
    
    if os.path.exists(f'{output_dir}/model_parameters.fits'):
        qsos_prev = Table.read(f'{output_dir}/model_parameters.fits')
        qsos = vstack([qsos_prev, qsos.data])
    else:
        qsos = qsos.data
    qsos.write(f'{output_dir}/model_parameters.fits', overwrite=True)

def main():
    from argparse import ArgumentParser
    parser = ArgumentParser('Simulate quasars without absorber templates')
    parser.add_argument("--number", type=int, default=None,
                        help="Number of quasars to simulate [default=100]")
    parser.add_argument("--zlist", type=str, default=None,
                        help="File containing list of redshifts to use (one per line)")
    parser.add_argument("--wavelen_grid", type=str, default='/data2/home2/nguerrav/TNG50_spec/npy_files/TNG50_wavelength_grid_extended.npy',
                        help="File with wavelength grid over which to simulate the QSOs")
    # parser.add_argument("--zmin", type=float, default=1.0,
    #                     help="Minimum redshift [default=1.0]")
    # parser.add_argument("--zmax", type=float, default=4.5,
    #                     help="Maximum redshift [default=4.5]")
    parser.add_argument("--wmin", type=float, default=3000,
                        help="Minimum wavelength in Angstrom [default=3000]")
    parser.add_argument("--wmax", type=float, default=11000,
                        help="Maximum wavelength in Angstrom [default=11000]")
    parser.add_argument('--dust', type=str, default='exponential',
                        help="Dust sampling mode: 'exponential' or 'uniform' [default=exponential]")
    # parser.add_argument('--bal', action='store_true',
    #                     help="Include broad absorption line features")
    parser.add_argument("--dir", type=str, default='/data2/home2/nguerrav/QSO_simpaqs/QSOs_full_cat',
                        help="Output directory")

    args = parser.parse_args()
    
    # print(f"Simulating {args.number} quasars without absorber templates")
    # print(f"Redshift range: {args.zmin} - {args.zmax}")
    print(f"Wavelength range: {args.wmin} - {args.wmax} Å")
    print(f"Dust mode: {args.dust}")
    # print(f"BAL features: {'Yes' if args.bal else 'No'}")
    print(f"Output directory: {args.dir}")

    # cat = Table.read('/data2/home2/nguerrav/Catalogues/ByCycle_Final_Cat_with_all_S17_cols.fits').to_pandas()
    # cat = Table.read('/data2/home2/nguerrav/Catalogues/ByCycle_Final_Cat_fobs.fits').to_pandas()
    cat = pandas_from_fits('/data2/home2/nguerrav/Catalogues/ByCycle_Final_Cat_fobs.fits')

    if args.number is not None:
        nqsos = args.number
    else:
        nqsos = cat.shape[0]
    print(f"Total number of quasars to simulate: {nqsos}")

    if nqsos > 50000:
        # Process in chunks to manage memory
        chunk_size = 10000
        num_chunks = (nqsos + chunk_size - 1) // chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min((chunk_idx + 1) * chunk_size, nqsos)
            
            print(f"\nProcessing chunk {chunk_idx+1}/{num_chunks} (QSOs {start_idx+1}-{end_idx})")
            
            chunk_cat = cat.iloc[start_idx:end_idx]
            z_arr = chunk_cat.REDSHIFT_ESTIMATE.values
            names = chunk_cat['NAME'].to_numpy(dtype=str)
            
            simulate_quasars(
                nqso=len(z_arr),
                z_list=z_arr, 
                names_list=names,
                wave_range=(args.wmin, args.wmax),
                dust_mode=args.dust,
                output_dir=args.dir
            )
            
            del chunk_cat, z_arr, names
            gc.collect()

    else:
        z_arr = cat.REDSHIFT_ESTIMATE.values
        names = cat['NAME'].to_numpy(dtype=str)

        simulate_quasars(
            nqso=args.number,
            # z_range=(args.zmin, args.zmax),
            z_list=z_arr, 
            names_list=names,
            wave_range=(args.wmin, args.wmax),
            dust_mode=args.dust,
            # BAL=args.bal,
            output_dir=args.dir
        )

    model_params = pd.read_csv(f'{args.dir}/model_parameters.csv')
    qso_name_to_id = dict(zip(model_params['NAME'], model_params['ID'] + '.fits'))
    cat['TEMPLATE'] = cat['NAME'].map(qso_name_to_id)

    save_to_fits(cat, '/data2/home2/nguerrav/Catalogues/ByCycle_Final_Cat_fobs_qso_templates.fits')

if __name__ == '__main__':
    main()
